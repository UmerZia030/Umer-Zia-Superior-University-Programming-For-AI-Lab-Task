{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bf27669",
   "metadata": {},
   "source": [
    "# text generation using fnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99fb56f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001EB0EC1DF40>, 'Connection to files.pythonhosted.org timed out. (connect timeout=15)')': /packages/e3/f5/668b3444a2f487b0052b908af631fe39eeb2bdb2359d9bbc2c3b80b71119/datasets-3.5.1-py3-none-any.whl.metadata\n",
      "  WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001EB0EC1FA40>, 'Connection to files.pythonhosted.org timed out. (connect timeout=15)')': /packages/e3/f5/668b3444a2f487b0052b908af631fe39eeb2bdb2359d9bbc2c3b80b71119/datasets-3.5.1-py3-none-any.whl.metadata\n",
      "  WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001EB0EC1F440>, 'Connection to files.pythonhosted.org timed out. (connect timeout=15)')': /packages/e3/f5/668b3444a2f487b0052b908af631fe39eeb2bdb2359d9bbc2c3b80b71119/datasets-3.5.1-py3-none-any.whl.metadata\n",
      "  WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001EB0EC1DDC0>, 'Connection to files.pythonhosted.org timed out. (connect timeout=15)')': /packages/e3/f5/668b3444a2f487b0052b908af631fe39eeb2bdb2359d9bbc2c3b80b71119/datasets-3.5.1-py3-none-any.whl.metadata\n",
      "  WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001EB0EC1ED80>, 'Connection to files.pythonhosted.org timed out. (connect timeout=15)')': /packages/e3/f5/668b3444a2f487b0052b908af631fe39eeb2bdb2359d9bbc2c3b80b71119/datasets-3.5.1-py3-none-any.whl.metadata\n",
      "ERROR: Could not install packages due to an OSError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Max retries exceeded with url: /packages/e3/f5/668b3444a2f487b0052b908af631fe39eeb2bdb2359d9bbc2c3b80b71119/datasets-3.5.1-py3-none-any.whl.metadata (Caused by ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001EB0EC1E000>, 'Connection to files.pythonhosted.org timed out. (connect timeout=15)'))\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed7a5470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001B786D5B200>, 'Connection to files.pythonhosted.org timed out. (connect timeout=15)')': /packages/e3/f5/668b3444a2f487b0052b908af631fe39eeb2bdb2359d9bbc2c3b80b71119/datasets-3.5.1-py3-none-any.whl.metadata\n",
      "  WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001B786D5ABA0>, 'Connection to files.pythonhosted.org timed out. (connect timeout=15)')': /packages/e3/f5/668b3444a2f487b0052b908af631fe39eeb2bdb2359d9bbc2c3b80b71119/datasets-3.5.1-py3-none-any.whl.metadata\n",
      "  WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001B786D5B440>, 'Connection to files.pythonhosted.org timed out. (connect timeout=15)')': /packages/e3/f5/668b3444a2f487b0052b908af631fe39eeb2bdb2359d9bbc2c3b80b71119/datasets-3.5.1-py3-none-any.whl.metadata\n",
      "  WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001B786D5A480>, 'Connection to files.pythonhosted.org timed out. (connect timeout=15)')': /packages/e3/f5/668b3444a2f487b0052b908af631fe39eeb2bdb2359d9bbc2c3b80b71119/datasets-3.5.1-py3-none-any.whl.metadata\n",
      "  WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001B786D5B0E0>, 'Connection to files.pythonhosted.org timed out. (connect timeout=15)')': /packages/e3/f5/668b3444a2f487b0052b908af631fe39eeb2bdb2359d9bbc2c3b80b71119/datasets-3.5.1-py3-none-any.whl.metadata\n",
      "ERROR: Could not install packages due to an OSError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Max retries exceeded with url: /packages/e3/f5/668b3444a2f487b0052b908af631fe39eeb2bdb2359d9bbc2c3b80b71119/datasets-3.5.1-py3-none-any.whl.metadata (Caused by ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001B786D5A2D0>, 'Connection to files.pythonhosted.org timed out. (connect timeout=15)'))\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch[transformers] in c:\\users\\jamshaid gill\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.7.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: torch 2.7.0 does not provide the extra 'transformers'\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: filelock in c:\\users\\jamshaid gill\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch[transformers]) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\jamshaid gill\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch[transformers]) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\jamshaid gill\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch[transformers]) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\jamshaid gill\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch[transformers]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jamshaid gill\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch[transformers]) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jamshaid gill\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch[transformers]) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jamshaid gill\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch[transformers]) (69.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\jamshaid gill\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch[transformers]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jamshaid gill\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch[transformers]) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install torch[transformers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 : Load Data\n",
    "from datasets import load_dataset\n",
    "datasets = load_dataset('wikitext','wikitext-2-raw-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Data Preprocessing\n",
    "import re\n",
    "\n",
    "\n",
    "def preprocess_text(sentence):\n",
    "    # lowering the sentence and storing in text vaiable\n",
    "    text = sentence['text'].lower()\n",
    "    # removing other than characters and punctuations\n",
    "    text = re.sub('[^a-z?!.,]', ' ', text)\n",
    "    text = re.sub('\\s\\s+', ' ', text)  # removing double spaces\n",
    "    sentence['text'] = text\n",
    "    return sentence\n",
    "\n",
    "\n",
    "datasets['train'] = datasets['train'].map(preprocess_text)\n",
    "datasets['test'] = datasets['test'].map(preprocess_text)\n",
    "datasets['validation'] = datasets['validation'].map(preprocess_text)\n",
    "\n",
    "datasets['train'] = datasets['train'].filter(lambda x: len(x['text']) > 20)\n",
    "datasets['test'] = datasets['test'].filter(lambda x: len(x['text']) > 20)\n",
    "datasets['validation'] = datasets['validation'].filter(\n",
    "    lambda x: len(x['text']) > 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3bbd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4. : Tokenisation\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "# Tokenizer\n",
    "def tokenize(sentence):\n",
    "    sentence = tokenizer(sentence['text'], truncation=True)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "tokenized_inputs = datasets['test'].map(tokenize)\n",
    "tokenized_inputs = tokenized_inputs.remove_columns(['text'])\n",
    "\n",
    "\n",
    "# DataCollator\n",
    "batch = 16\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer, padding=True, return_tensors=\"pt\")\n",
    "dataloader = DataLoader(\n",
    "    tokenized_inputs, batch_size=batch, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e57b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 : Embedding Positional encoding\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.fft as fft\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.positional_encoding = self.create_positional_encoding().to(device)\n",
    "\n",
    "    def create_positional_encoding(self):\n",
    "\n",
    "        # Initialize positional encoding matrix\n",
    "        positional_encoding = np.zeros((self.max_sequence_length, self.d_model))\n",
    "\n",
    "        # Calculate positional encoding for each position and each dimension\n",
    "        for pos in range(self.max_sequence_length):\n",
    "            for i in range(0, self.d_model, 2):\n",
    "                # Apply sin to even indices in the array; indices in Python start at 0 so i is even.\n",
    "                positional_encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i) / self.d_model)))\n",
    "\n",
    "                if i + 1 < self.d_model:\n",
    "                    # Apply cos to odd indices in the array; we add 1 to i because indices in Python start at 0.\n",
    "                    positional_encoding[pos, i + 1] = np.cos(pos / (10000 ** ((2 * i) / self.d_model)))\n",
    "\n",
    "        # Convert numpy array to PyTorch tensor and return it\n",
    "        return torch.from_numpy(positional_encoding).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        expanded_tensor = torch.unsqueeze(self.positional_encoding, 0).expand(x.size(0), -1, -1).to(device)\n",
    "\n",
    "        return x.to(device) + expanded_tensor[:,:x.size(1), :]\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "  def __init__(self, sequence_length, vocab_size, embed_dim):\n",
    "    super(PositionalEmbedding, self).__init__()\n",
    "    self.token_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "    self.position_embeddings = PositionalEncoding(embed_dim,sequence_length)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    embedded_tokens = self.token_embeddings(inputs).to(device)\n",
    "    embedded_positions = self.position_embeddings(embedded_tokens).to(device)\n",
    "    return embedded_positions.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02c58c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6 : Create FNet Encoder\n",
    "class FNetEncoder(nn.Module):\n",
    "\n",
    "  def __init__(self,embed_dim, dense_dim):\n",
    "    super(FNetEncoder,self).__init__()\n",
    "    self.embed_dim = embed_dim\n",
    "    self.dense_dim = dense_dim\n",
    "    self.dense_proj = nn.Sequential(nn.Linear(self.embed_dim,self.dense_dim), nn.ReLU(), nn.Linear(self.dense_dim,self.embed_dim))\n",
    "\n",
    "    self.layernorm_1 = nn.LayerNorm(self.embed_dim)\n",
    "    self.layernorm_2 = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "  def forward(self,inputs):\n",
    "\n",
    "    fft_result = fft.fft2(inputs)\n",
    "\n",
    "    #taking real part\n",
    "    fft_real = fft_result.real.float()\n",
    "\n",
    "    proj_input = self.layernorm_1 (inputs + fft_real)\n",
    "    proj_output = self.dense_proj(proj_input)\n",
    "    return self.layernorm_2(proj_input +proj_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4866c9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7 : Create FnetDecoder\n",
    "class FNetDecoder(nn.Module):\n",
    "\n",
    "  def __init__(self,embed_dim,dense_dim,num_heads):\n",
    "    super(FNetDecoder,self).__init__()\n",
    "    self.embed_dim = embed_dim\n",
    "    self.dense_dim = dense_dim\n",
    "    self.num_heads = num_heads\n",
    "\n",
    "    self.attention_1 = nn.MultiheadAttention(embed_dim,num_heads,batch_first=True)\n",
    "    self.attention_2 = nn.MultiheadAttention(embed_dim,num_heads,batch_first=True)\n",
    "\n",
    "    self.dense_proj = nn.Sequential(nn.Linear(embed_dim, dense_dim),nn.ReLU(),nn.Linear(dense_dim, embed_dim))\n",
    "\n",
    "    self.layernorm_1 = nn.LayerNorm(embed_dim)\n",
    "    self.layernorm_2 = nn.LayerNorm(embed_dim)\n",
    "    self.layernorm_3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "  def forward(self, inputs, encoder_outputs, mask=None):\n",
    "    causal_mask = nn.Transformer.generate_square_subsequent_mask(inputs.size(1)).to(device)\n",
    "\n",
    "    attention_output_1, _ = self.attention_1(inputs, inputs, inputs, attn_mask=causal_mask)\n",
    "    out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "    if mask != None:\n",
    "      attention_output_2, _ = self.attention_2(out_1, encoder_outputs, encoder_outputs, key_padding_mask =torch.transpose(mask, 0, 1).to(device))\n",
    "    else:\n",
    "      attention_output_2, _ = self.attention_2(out_1, encoder_outputs, encoder_outputs)\n",
    "    out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "    proj_output = self.dense_proj(out_2)\n",
    "    return self.layernorm_3(out_2 + proj_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0034b023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8 : Fnet Model\n",
    "class FNetModel(nn.Module):\n",
    "    def __init__(self, max_length, vocab_size, embed_dim, latent_dim, num_heads):\n",
    "        super(FNetModel, self).__init__()\n",
    "\n",
    "        self.encoder_inputs = PositionalEmbedding(max_length,vocab_size, embed_dim)\n",
    "        self.encoder1 = FNetEncoder(embed_dim, latent_dim)\n",
    "        self.encoder2 = FNetEncoder(embed_dim, latent_dim)\n",
    "        self.encoder3 = FNetEncoder(embed_dim, latent_dim)\n",
    "        self.encoder4 = FNetEncoder(embed_dim, latent_dim)\n",
    "\n",
    "\n",
    "        self.decoder_inputs = PositionalEmbedding(max_length,vocab_size, embed_dim)\n",
    "        self.decoder1 = FNetDecoder(embed_dim, latent_dim, num_heads)\n",
    "        self.decoder2 = FNetDecoder(embed_dim, latent_dim, num_heads)\n",
    "        self.decoder3 = FNetDecoder(embed_dim, latent_dim, num_heads)\n",
    "        self.decoder4 = FNetDecoder(embed_dim, latent_dim, num_heads)\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.dense = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def encoder(self,encoder_inputs):\n",
    "        x_encoder = self.encoder_inputs(encoder_inputs)\n",
    "        x_encoder = self.encoder1(x_encoder)\n",
    "        x_encoder = self.encoder2(x_encoder)\n",
    "        x_encoder = self.encoder3(x_encoder)\n",
    "        x_encoder = self.encoder4(x_encoder)\n",
    "        return x_encoder\n",
    "\n",
    "    def decoder(self,decoder_inputs,encoder_output,att_mask):\n",
    "        x_decoder = self.decoder_inputs(decoder_inputs)\n",
    "        x_decoder = self.decoder1(x_decoder, encoder_output,att_mask) ## HERE for inference\n",
    "        x_decoder = self.decoder2(x_decoder, encoder_output,att_mask) ## HERE for inference\n",
    "        x_decoder = self.decoder3(x_decoder, encoder_output,att_mask) ## HERE for inference\n",
    "        x_decoder = self.decoder4(x_decoder, encoder_output,att_mask) ## HERE for inference\n",
    "        decoder_outputs = self.dense(x_decoder)\n",
    "\n",
    "        return decoder_outputs\n",
    "\n",
    "    def forward(self, encoder_inputs, decoder_inputs,att_mask = None):\n",
    "        encoder_output = self.encoder(encoder_inputs)\n",
    "        decoder_output = self.decoder(decoder_inputs,encoder_output,att_mask=None)\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba792b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9 : Initialize Model\n",
    "# Assuming your constants are defined like this:\n",
    "MAX_LENGTH = 512\n",
    "VOCAB_SIZE = len(tokenizer.vocab)\n",
    "EMBED_DIM = 256\n",
    "LATENT_DIM = 100\n",
    "NUM_HEADS = 4\n",
    "\n",
    "# Create an instance of the model\n",
    "fnet_model = FNetModel(MAX_LENGTH, VOCAB_SIZE, EMBED_DIM, LATENT_DIM, NUM_HEADS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10 : Train the model\n",
    "# Define your optimizer and loss function\n",
    "optimizer = torch.optim.Adam(fnet_model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    for batch in dataloader:\n",
    "        encoder_inputs_tensor = batch['input_ids'][:,:-1].to(device)\n",
    "        decoder_inputs_tensor = batch['input_ids'][:,1:].to(device)\n",
    "\n",
    "        att_mask = batch['attention_mask'][:,:-1].to(device).to(dtype=bool)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = fnet_model(encoder_inputs_tensor, decoder_inputs_tensor,att_mask)\n",
    "        decoder_inputs_tensor.masked_fill(batch['attention_mask'][:,1:].ne(1).to(device), -100).to(device)\n",
    "\n",
    "        loss = criterion(outputs.view(-1, VOCAB_SIZE), decoder_inputs_tensor.reshape(-1))\n",
    "        train_loss = train_loss + loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print (f\" epoch: {epoch}, train_loss : {train_loss}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
